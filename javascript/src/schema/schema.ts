/**
 * The encoding of the audio.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "AudioEncoding".
 */
export type AudioEncoding =
  | "linear16"
  | "flac"
  | "mulaw"
  | "alaw"
  | "aac"
  | "mp3"
  | "opus";
/**
 * The container format of the audio.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "AudioContainer".
 */
export type AudioContainer = "wav" | "ogg" | "flac" | "webm";
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "Part".
 */
export type Part =
  | TextPart
  | ImagePart
  | AudioPart
  | ToolCallPart
  | ToolResultPart;
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "Message".
 */
export type Message = UserMessage | AssistantMessage | ToolMessage;
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "Modality".
 */
export type Modality = "text" | "audio";

export interface LlmSdk {
  [k: string]: unknown;
}
/**
 * A part of the message that contains text.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "TextPart".
 */
export interface TextPart {
  type: "text";
  text: string;
  /**
   * The optional ID of the part.
   */
  id?: string;
}
/**
 * A part of the message that contains an image.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ImagePart".
 */
export interface ImagePart {
  type: "image";
  /**
   * The MIME type of the image. E.g. "image/jpeg", "image/png".
   */
  mimeType: string;
  /**
   * The base64-encoded image data.
   */
  imageData: string;
  /**
   * The width of the image in pixels.
   */
  width?: number;
  /**
   * The height of the image in pixels.
   */
  height?: number;
  /**
   * The optional ID of the part.
   */
  id?: string;
}
/**
 * A part of the message that contains an audio.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "AudioPart".
 */
export interface AudioPart {
  type: "audio";
  container?: AudioContainer;
  /**
   * The base64-encoded audio data.
   */
  audioData: string;
  encoding?: AudioEncoding;
  /**
   * The sample rate of the audio. E.g. 44100, 48000.
   */
  sampleRate?: number;
  /**
   * The number of channels of the audio. E.g. 1, 2.
   */
  channels?: number;
  /**
   * The transcript of the audio.
   */
  transcript?: string;
  /**
   * The optional ID of the part.
   */
  id?: string;
}
/**
 * A part of the message that represents a call to a tool the model wants to use.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolCallPart".
 */
export interface ToolCallPart {
  type: "tool-call";
  /**
   * The ID of the tool call, used to match the tool result with the tool call.
   */
  toolCallId: string;
  /**
   * The name of the tool to call.
   */
  toolName: string;
  /**
   * The arguments to pass to the tool.
   */
  args: {
    [k: string]: unknown;
  } | null;
  /**
   * The optional ID of the part. This might not be the same as the toolCallId.
   */
  id?: string;
}
/**
 * A part of the message that represents the result of a tool call.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolResultPart".
 */
export interface ToolResultPart {
  type: "tool-result";
  /**
   * The ID of the tool call from previous assistant message.
   */
  toolCallId: string;
  /**
   * The name of the tool that was called.
   */
  toolName: string;
  /**
   * The result of the tool call.
   */
  result:
    | {
        [k: string]: unknown;
      }
    | unknown[];
  /**
   * Marks the tool result as an error.
   */
  isError?: boolean;
}
/**
 * Represents a message sent by the user.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "UserMessage".
 */
export interface UserMessage {
  role: "user";
  content: (TextPart | ImagePart | AudioPart)[];
}
/**
 * Represents a message generated by the model.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "AssistantMessage".
 */
export interface AssistantMessage {
  role: "assistant";
  content: (TextPart | ToolCallPart | AudioPart)[];
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "TextPartDelta".
 */
export interface TextPartDelta {
  type: "text";
  text: string;
  /**
   * The optional ID of the part.
   */
  id?: string;
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolCallPartDelta".
 */
export interface ToolCallPartDelta {
  type: "tool-call";
  /**
   * The ID of the tool call, used to match the tool result with the tool call.
   */
  toolCallId?: string;
  /**
   * The name of the tool to call.
   */
  toolName?: string;
  /**
   * The partial JSON string of the arguments to pass to the tool.
   */
  args?: string;
  /**
   * The optional ID of the part. This might not be the same as the toolCallId.
   */
  id?: string;
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "AudioPartDelta".
 */
export interface AudioPartDelta {
  type: "audio";
  /**
   * The base64-encoded audio data.
   */
  audioData?: string;
  container?: AudioContainer;
  encoding?: AudioEncoding;
  /**
   * The sample rate of the audio. E.g. 44100, 48000.
   */
  sampleRate?: number;
  /**
   * The number of channels of the audio. E.g. 1, 2.
   */
  channels?: number;
  /**
   * The transcript of the audio.
   */
  transcript?: string;
  /**
   * The optional ID of the part.
   */
  id?: string;
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ContentDelta".
 */
export interface ContentDelta {
  index: number;
  part: TextPartDelta | ToolCallPartDelta | AudioPartDelta;
}
/**
 * Represents a JSON schema.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "JSONSchema".
 */
export interface JSONSchema {
  [k: string]: unknown;
}
/**
 * Represents a tool that can be used by the model.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "Tool".
 */
export interface Tool {
  /**
   * The name of the tool.
   */
  name: string;
  /**
   * A description of the tool.
   */
  description: string;
  /**
   * The JSON schema of the parameters that the tool accepts. The type must be "object".
   */
  parameters: JSONSchema | null;
}
/**
 * Represents tool result in the message history.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolMessage".
 */
export interface ToolMessage {
  role: "tool";
  content: ToolResultPart[];
}
/**
 * Represents the token usage of the model.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ModelTokensDetail".
 */
export interface ModelTokensDetail {
  textTokens?: number;
  audioTokens?: number;
  imageTokens?: number;
}
/**
 * Represents the token usage of the model.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ModelUsage".
 */
export interface ModelUsage {
  inputTokens: number;
  outputTokens: number;
  inputTokensDetail?: ModelTokensDetail;
  outputTokensDetail?: ModelTokensDetail;
}
/**
 * Represents the response generated by the model.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ModelResponse".
 */
export interface ModelResponse {
  content: (TextPart | ToolCallPart | AudioPart)[];
  usage?: ModelUsage;
  /**
   * The cost of the response.
   */
  cost?: number;
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "PartialModelResponse".
 */
export interface PartialModelResponse {
  delta: ContentDelta;
}
/**
 * The model will automatically choose the tool to use or not use any tools.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolChoiceAuto".
 */
export interface ToolChoiceAuto {
  type: "auto";
}
/**
 * The model will not use any tools.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolChoiceNone".
 */
export interface ToolChoiceNone {
  type: "none";
}
/**
 * The model will be forced to use a tool.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolChoiceRequired".
 */
export interface ToolChoiceRequired {
  type: "required";
}
/**
 * The model will use the specified tool.
 *
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ToolChoiceTool".
 */
export interface ToolChoiceTool {
  type: "tool";
  toolName: string;
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ResponseFormatText".
 */
export interface ResponseFormatText {
  type: "text";
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "ResponseFormatJson".
 */
export interface ResponseFormatJson {
  type: "json";
  schema?: JSONSchema;
}
/**
 * This interface was referenced by `LlmSdk`'s JSON-Schema
 * via the `definition` "LanguageModelInput".
 */
export interface LanguageModelInput {
  /**
   * A system prompt is a way of providing context and instructions to the model
   */
  systemPrompt?: string;
  /**
   * A list of messages comprising the conversation so far.
   */
  messages: Message[];
  /**
   * Definitions of tools that the model may use.
   */
  tools?: Tool[];
  /**
   * Determines how the model should choose which tool to use. "auto" - The model will automatically choose the tool to use or not use any tools. "none" - The model will not use any tools. "required" - The model will be forced to use a tool. { type: "tool", toolName: "toolName" } - The model will use the specified tool.
   */
  toolChoice?:
    | ToolChoiceAuto
    | ToolChoiceNone
    | ToolChoiceRequired
    | ToolChoiceTool;
  /**
   * The format that the model must output
   */
  responseFormat?: ResponseFormatJson | ResponseFormatText;
  /**
   * The maximum number of tokens that can be generated in the chat completion.
   */
  maxTokens?: number;
  /**
   * Amount of randomness injected into the response. Ranges from 0.0 to 1.0
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass Ranges from 0.0 to 1.0
   */
  topP?: number;
  /**
   * Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Ranges from 0.0 to 1.0
   */
  topK?: number;
  /**
   * Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   */
  presencePenalty?: number;
  /**
   * Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   */
  frequencyPenalty?: number;
  /**
   * The seed (integer), if set and supported by the model, to enable deterministic results.
   */
  seed?: number;
  /**
   * The modalities that the model should support.
   */
  modalities?: Modality[];
  /**
   * Extra options that the model may support.
   */
  extra?: {
    [k: string]: unknown;
  };
}
