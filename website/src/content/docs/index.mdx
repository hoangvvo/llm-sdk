---
title: llm-sdk
---

import { CardGrid, LinkCard } from '@astrojs/starlight/components';
import { Image } from 'astro:assets';
import imageConsoleChat from "../../../assets/console-chat.png";

<CardGrid>
  <LinkCard
    title="Get started with Agent"
    description="Learn how to build LLM applications using llm-agent."
    href="/agent/"
  />
  <LinkCard
    title="Get started with SDK"
    description="Use various LLM providers with a unified interface using llm-sdk."
    href="/sdk/"
  />
</CardGrid>

`llm-sdk` is a suite of libraries to interact with various Large Language Model (LLM) providers through a unified API and build LLM applications.

Two libraries are provided:

- [LLM SDK](./sdk/): Unified SDKs to interact with various LLM providers.
- [LLM Agent](./agent/): An abstraction to build LLM applications using the LLM SDK.

Check out the [Console Application](/console/chat/) for a demo application that showcases the capabilities of the libraries.

<Image src={imageConsoleChat} alt="Console Chat UI" quality="high" />

## Features

- Supports multiple LLM providers with a unified API.
- Handles multiple modalities: Text, Image, and Audio.
- Supports streaming, including for image and audio.
- Supports multi-modality tool results (image/audio returned from tools).
- Supports citations (RAG) and reasoning for supported models.
- Reports token usage and calculates the cost of a request when provided with the model's pricing information.
- Unified serialization across JS, Rust, and Go (systems in different languages can work together).
- Integrates OpenTelemetry for tracing.
