# llm-sdk

Access the LLM API of different providers using a unified SDK.

## Features

- Supports multiple LLM providers with a unified API.
- Supports multi modalities: Text, Image, and Audio, even in stream requests.
- Supports function calling.
- Supports streaming responses. Contains utilities for building final output from streamed data, including for streaming audio.
- Supports reporting the token usage and calculating cost of a request if given the model's pricing information.
- Provides consistent serialization and deserialization for data storage across different programming languages.

## Specification

The specification serves as the basis to implement the unified LLM SDK in different programming languages. The specification is expressed using TypeScript in [schemas/sdk.ts](./schema/sdk.ts).

Implementations in different programming languages should adhere to this specification strictly. In particular, the properties in data structures should have the same names and types, even if that goes against the conventions of the target language, such as camelCase vs snake_case (we follow the later).

Each implementation may provide additional features.

## LLM SDKs

We provide SDKs to interact with various LLM providers in the following programming languages:

- [JavaScript](./sdk-js/README.md)
- [Rust](./sdk-rust/README.md)

### Supported Providers

| Feature \ Provider                | OpenAI            | Anthropic                                                 | Google | Cohere | Mistral           |
| --------------------------------- | ----------------- | --------------------------------------------------------- | ------ | ------ | ----------------- |
| Sampling Params[^sampling_params] | âœ… except `top_k` | âœ… except `frequency_penalty`, `presence_penalty`, `seed` | âœ…     | âœ…     | âœ… except `top_k` |
| Function Calling                  | âœ…                | âœ…                                                        | âœ…     | âœ…     | âœ…                |
| Structured Output                 | âœ…                | âž–                                                        | âœ…     | âœ…     | âœ…                |
| Text Input                        | âœ…                | âœ…                                                        | âœ…     | âœ…     | âœ…                |
| Image Input                       | âœ…                | âœ…                                                        | âœ…     | âœ…     | âœ…                |
| Audio Input                       | âœ…                | âž–                                                        | âž–     | âž–     | âž–                |
| Text Output                       | âœ…                | âœ…                                                        | âœ…     | âœ…     | âœ…                |
| Image Output                      | ðŸš§                | âž–                                                        | âœ…     | âž–     | âž–                |
| Audio Output                      | âœ…                | âž–                                                        | âž–     | âž–     | âž–                |

Keys:

- âœ…: Supported
- ðŸš§: Not yet implemented
- âž–: Not available from provider

[^sampling_params]: The following sampling parameters are supported: `max_tokens`, `temperature`, `top_p`, `top_k`, `presence_penalty`, `frequency_penalty`, `seed`

### Message

`messages` are primitives which make up the conversation history, and `parts`, which are the building blocks of each message. The library converts them into a format suitable for the underlying LLM provider as well as mapping those from different providers to the unified format.

The following `Message` types are implemented in the SDK:

```ts
/**
 * Represents a message sent by the user.
 */
export interface UserMessage {
  role: "user";
  content: Part[];
}

/**
 * Represents a message generated by the model.
 */
export interface AssistantMessage {
  role: "assistant";
  content: Part[];
}

/**
 * Represents tool result in the message history.
 * Only ToolResultPart should be included in the content.
 */
export interface ToolMessage {
  role: "tool";
  content: Part[];
}
```

### Part

> [!NOTE]
> Tool calls are implemented as a `Part` instead of on a property of the `AssistantMessage`

> [!NOTE]
> `ToolResultPart` content is an array of `Part` instead of a string or an object, which enables non-text results to be returned for LLM provider that supports them (e.g. Anthropic Function Calling supports Image in Tool Result)

The following `Part` types are implemented in the SDK:

```ts
/**
 * A part of the message that contains text.
 */
export interface TextPart {
  type: "text";
  text: string;
  /**
   * The ID of the part, if applicable.
   */
  id?: string;
}

/**
 * A part of the message that contains an image.
 */
export interface ImagePart {
  type: "image";
  /**
   * The MIME type of the image. E.g. "image/jpeg", "image/png".
   */
  mime_type: string;
  /**
   * The base64-encoded image data.
   */
  image_data: string;
  /**
   * The width of the image in pixels.
   */
  width?: number;
  /**
   * The height of the image in pixels.
   */
  height?: number;
  /**
   * The ID of the part, if applicable.
   */
  id?: string;
}

/**
 * A part of the message that contains an audio.
 */
export interface AudioPart {
  type: "audio";
  /**
   * The base64-encoded audio data.
   */
  audio_data: string;
  format?: AudioFormat;
  /**
   * The sample rate of the audio. E.g. 44100, 48000.
   */
  sample_rate?: number;
  /**
   * The number of channels of the audio. E.g. 1, 2.
   */
  channels?: number;
  /**
   * The transcript of the audio.
   */
  transcript?: string;
  /**
   * The ID of the part, if applicable.
   */
  id?: string;
}

/**
 * A part of the message that contains an audio.
 */
export interface AudioPart {
  type: "audio";
  /**
   * The base64-encoded audio data.
   */
  audio_data: string;
  format?: AudioFormat;
  /**
   * The sample rate of the audio. E.g. 44100, 48000.
   */
  sample_rate?: number;
  /**
   * The number of channels of the audio. E.g. 1, 2.
   */
  channels?: number;
  /**
   * The transcript of the audio.
   */
  transcript?: string;
  /**
   * The ID of the part, if applicable.
   */
  id?: string;
}
/**
 * A part of the message that represents a call to a tool the model wants to use.
 */
export interface ToolCallPart {
  type: "tool-call";
  /**
   * The ID of the tool call, used to match the tool result with the tool call.
   */
  tool_call_id: string;
  /**
   * The name of the tool to call.
   */
  tool_name: string;
  /**
   * The arguments to pass to the tool.
   */
  args: {
    [k: string]: unknown;
  };
  /**
   * The ID of the part, if applicable. This might not be the same as the tool_call_id.
   */
  id?: string;
}

/**
 * A part of the message that represents the result of a tool call.
 */
export interface ToolResultPart {
  type: "tool-result";
  /**
   * The ID of the tool call from previous assistant message.
   */
  tool_call_id: string;
  /**
   * The name of the tool that was called.
   */
  tool_name: string;
  /**
   * The content of the tool result.
   */
  content: Part[];
  /**
   * Marks the tool result as an error.
   */
  is_error?: boolean;
}
```

For streaming call, there are also the counterparts `XXXPartDelta`.

## License

[MIT](LICENSE)
