---
title: Agent
---

import { Aside, Tabs, TabItem, Code } from "@astrojs/starlight/components";
import jsAgentCode from "../../../../../agent-js/examples/agent.ts?raw";
import rustAgentCode from "../../../../../agent-rust/examples/agent.rs?raw";
import goAgentCode from "../../../../../agent-go/examples/agent/main.go?raw";

Below is an example of how to implement an agent in each language:

<Tabs>
  <TabItem label="TypeScript">
    <Code code={jsAgentCode} lang="typescript" title="agent.ts" />
  </TabItem>
  <TabItem label="Rust">
    <Code code={rustAgentCode} lang="rust" title="agent.rs" />
  </TabItem>
  <TabItem label="Go">
    <Code code={goAgentCode} lang="go" title="main.go" />
  </TabItem>
</Tabs>

## Agent Definition

An agent is constructed with the following parameters:

- `name`: The identifier of the agent.
- `model`: The language model instance from the LLM SDK.
- `instructions`: A list of instructions to be injected into the system prompt to guide the agent's behavior.
- `tools`: A list of _executable_ tools that the agent can call during its execution.
- `response_format`: The expected response format from the agent. While the default is plain text, it can be customized to return structured output.
- `max_turns`: The maximum number of turns the agent can take to complete a request.
- Other sampling parameters: `temperature`, `top_p`, `max_tokens`, etc.

In addition, the agent is defined with a `context` generic type that can be accessed in the instructions (for dynamic instructions) and tools.

## Agent Tools

An agent tool is defined with the following properties:

- `name`: The identifier of the tool.
- `description`: A description of the tool to instruct the model how and when to use it.
- `parameters`: The JSON schema of the parameters that the tool accepts. The type must be "object".
- `execute(args, context, state)`: The function that will be called to execute the tool with given parameters and context.

The `execute` function must always return an `AgentToolResult`, which includes:

- `content`: The content generated by the tool, which is an array of `Part`, allowing multi-modal outputs for language models that support them.
- `is_error`: A boolean indicating whether the tool execution resulted in an error. Some language models utilize this property to guide its behavior.

## Agent Run

An agent run is initiated by calling the `run` method with an `AgentRequest`, which includes the following:

- `input`: The list of input `AgentItem` for the agent, such as `Message`s.
- `context`: A user-provided value that can be accessed in instructions and tools.

```typescript
/**
 * An input for or output entry generated by the agent run
 */
export type AgentItem = AgentItemMessage;

/**
 * A LLM message used in the run
 */
export type AgentItemMessage = { type: "message" } & Message;
```

<Aside>
  Each agent run is stateless, so it is recommended to implement a strategy to
  persist the conversation history if needed.
</Aside>

Each run will continuously generate LLM completions, parse responses to check for tool calls, execute any tools, and feed the tool results back to the model until one of the following conditions is met:

- The model generates a final response (i.e., no tool call).
- The maximum number of turns is reached.

`AgentResponse` includes the final response with the following properties:

- `output`: A list of output `AgentItem`, such as `ToolMessage` and `AssistantMessage`, that were generated during the run. This can be used to append to the `input` of the next run.
- `content`: The final content generated by the agent, which is usually the content of the last `AssistantMessage`.
- `model_calls`: A list of `ModelCallInfo` describing the LLM calls including their usage and cost.

```typescript
export interface AgentResponse {
  /**
   * The items generated during the agent's execution, such as new ToolMessage and AssistantMessage
   */
  output: AgentItem[];

  /**
   * The final output content generated by the agent.
   */
  content: Part[];

  /**
   * Model calls information, including usage and cost
   */
  model_calls: ModelCallInfo[];
}

/**
 * Track a call to the LLM and record its usage and cost.
 */
export interface ModelCallInfo {
  usage: ModelUsage | null;
  cost: number | null;
  model_id: string;
  provider: string;
}
```

The library also provides a streaming interface, similar to streaming LLM completions, to stream the agent run progress, including part deltas (e.g., text deltas, audio deltas) and intermediate tool calls. Each event can either be:

- `AgentStreamEventPartial`: Contains the `PartialModelResponse` as generated by the LLM SDK, which includes part deltas.
- `AgentStreamEventMessage`: Contains a full `Message` (e.g., `ToolMessage` or `AssistantMessage`) that was generated during the run.
- `AgentStreamEventResponse`: The final response of the agent run, which includes the `AgentResponse`.

```typescript
/**
 * Stream events emitted by the agent during runStream call.
 */
export type AgentStreamEvent =
  | AgentStreamEventPartial
  | AgentStreamEventMessage
  | AgentStreamEventResponse;

export interface AgentStreamEventPartial extends PartialModelResponse {
  type: "partial";
}

export type AgentStreamEventMessage = {
  type: "message";
} & Message;

export interface AgentStreamEventResponse extends AgentResponse {
  type: "response";
}
```

## Agent Patterns

Unlike other agent frameworks, this library does not hardcode any hidden prompt templates or parsing logic. This design choice avoids unnecessary constraints, reduces complexity, and prevents the loss of control that comes with excessive abstraction (see this [blog post](https://hamel.dev/blog/posts/prompt/) for a deeper discussion).

However, this prevents the library from implementing specific agent patterns, such as hand-off, memory, and others. The examples folder in each agent library contains implementations of various agent patterns to serve as references.
